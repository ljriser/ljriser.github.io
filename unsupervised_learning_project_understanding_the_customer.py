# -*- coding: utf-8 -*-
"""Unsupervised_Learning_Project_Understanding_the_Customer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jIRFjhlSPLWEx2WCtBmd07Y5-6PliwG1

# Unsupervised Learning Project: Understanding the Customer

by Landon Riser
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import scipy
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn import datasets, metrics
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.mixture import GaussianMixture
from sqlalchemy import create_engine
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
!pip install umap-learn
import umap
import time

# Code to read csv file into Colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

import warnings
warnings.filterwarnings('ignore')

# 'https://drive.google.com/file/d/1d_jkmjLMCo4Rrmdj8A7Vs3aXUVY-LiG-/view?usp=sharing'
fileDownloaded = drive.CreateFile({'id': '1d_jkmjLMCo4Rrmdj8A7Vs3aXUVY-LiG-'})
fileDownloaded.GetContentFile('customers.csv')

"""# Intoduction to the Dataset:
The customer dataset has 7 usable features from which to model and cluster in order to better understand and serve the customer base. There is no ground truth for which to judge clustering accuracy in this dataset, there is only inference and comparison of models to use for gauging effectiveness.

The features are as follows:


*   Gender: male(0) and female(1)
*   Marital status: single(0) and non-single (1)
*   Age: integer ranging from 18 to 76
*   Education: other/unknown(0), high school(1), university(2), graduate school (3)
*   Income: integer ranging from 35832 to 309364
*   Occupation: unemployed/unskilled(0), skilled employee (1), management/self-employed (2)
*   City size: small city (0), mid-sized city(1), big city (2)

"""

customers = pd.read_csv('customers.csv')

customers.info()

customers.head()

columns = ['sex', 'marital_status', 'age', 'education', 'income', 'occupation', 'city_size']
plt.figure(figsize=(24,16))
for i, column in enumerate(columns):
    plt.subplot(4,3,i+1)
    plt.hist(customers[column])
    plt.title(f"Histogram of {column}")
plt.show()

"""Looking at the histograms, we see a balance in both sex and marital status. Age is skewed toward the younger population and education level is dominated by high school educated(1) with an even split between less than high school educated(0) and college educated(2). Graduate school is represented by 3. Income takes on a similar distribution shape to age - a leftward skew, but with a central tendancy still above 100,000 USD. 
Occupation is made up of about 30% unemployed/unskilled workers, 60% skilled employees, and 10% management/self-employed/officer.
About half of the customers are in small cities, a quarter are from mid-sized cities and a quarter are from big cities.
"""

# Defining X as all rows and all features except for customer ID number
X = customers.iloc[:, 1:]

print(X.shape)

"""**Standardizing the data with the StandardScaler function**"""

# Standarizing the features
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

"""# Dimensonality Reduction

**PCA**
"""

# We just want the first two principal components
pca = PCA(n_components=2)

# We get the components by calling fit_transform method with our data
pca_components = pca.fit_transform(X_std)

pca_components.shape[0]

plt.figure(figsize=(10,5))
plt.scatter(pca_components[:, 0], pca_components[:, 1])
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.show()

"""PCA is insufficient to determine clusters for this data set. We can see a set of horizontal striation patterns, but besides the bottom cluster, the rest of the customer data points are difficult to distinguish into discrete clusters. We know that PCA is weak in retaining local similarities and this is an illustration of that.

**t-SNE**
"""

time_start = time.time()
perplexity = [value for value in range(10, 61, 10)]
plt.figure(figsize=(27,10))
for i, perp in enumerate(perplexity):
  
  tsne = TSNE(n_components=2, verbose=1, perplexity=perp, n_iter=2000)
  tsne_results = tsne.fit_transform(X_std)

  plt.subplot(2,3,i+1)
  plt.scatter(tsne_results[:, 0], tsne_results[:, 1])
  plt.xticks([])
  plt.yticks([])
  plt.axis('off')
  plt.title(f"Perplexity = {perp}")
plt.show()

print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))

"""The t-SNE technique is an expected improvement over PCA as we can see more distinct clusters. Increasing perplexity puts more space between individual clusters up until a perplexity value of 60(slightly beyond the suggested range) where it appears we level off.

**UMAP**

**Iterating on n_neighbors**
"""

time_start = time.time()
neighbors = [5, 10, 20, 40, 80, 160, 320, 500]
plt.figure(figsize=(27,15))
for i, neighbor in enumerate(neighbors):
  
  umap_results = umap.UMAP(n_neighbors=neighbor,
                      min_dist=0.3,
                      metric='correlation').fit_transform(X_std)

  plt.subplot(3,3,i+1)
  plt.scatter(umap_results[:, 0], umap_results[:, 1])
  plt.xticks([])
  plt.yticks([])
  plt.axis('off')
  plt.title(f"n_neighbors = {neighbor}")
plt.show()
print('UMAP done! Time elapsed: {} seconds'.format(time.time()-time_start))

"""The above figure shows a series of 2-component UMAP representations of our dataset where we sequentially double the value of n_neighbors while holding the other parameters constant. We begin to see a change in the global structure of the data after n_neighbors = 40.

**Iterating on min_dist**
"""

time_start = time.time()
distance = [0.1, 0.3, 0.5, 0.7, 0.9]
plt.figure(figsize=(27,15))
for i, dist in enumerate(distance):
  
  umap_results = umap.UMAP(n_neighbors=40,
                      min_dist=dist,
                      metric='correlation').fit_transform(X_std)

  plt.subplot(3,3,i+1)
  plt.scatter(umap_results[:, 0], umap_results[:, 1])
  plt.xticks([])
  plt.yticks([])
  plt.axis('off')
  plt.title(f"min_dist = {dist}")
plt.show()
print('UMAP done! Time elapsed: {} seconds'.format(time.time()-time_start))

"""Above we iterate on min_distance (holding other parameters constant) and observe that for the smaller values, we see broader spaced groupings of local similarities and as the value increases we see tighter spaced and larger groupings. A value of 0.5 with n_neighbors set to 40 appears to be the optimal parameter set in terms of visualizing the clusters and balancing the preservation of local and global structure of the broader dataset.

# Clustering

**K-means Clustering**
"""

# Computing the first principal components
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_std)

X_half1, X_half2, X_pcahalf1, X_pcahalf2 = train_test_split(
    X_std,
    X_pca,
    test_size=0.5,
    random_state=12)

preds = []
clusters = [item for item in range(5, 71, 5)]
for i, cluster in enumerate(clusters):
  pred = KMeans(n_clusters=cluster, random_state=12).fit_predict(X_std)
  print(f"{i+1}) Silhouette score for {cluster}-cluster K-means: {metrics.silhouette_score(X_std, pred, metric='euclidean')}")
  preds.append(metrics.silhouette_score(X_std, pred, metric='euclidean'))

plt.figure(figsize=(9,5))
plt.scatter(clusters,preds)
plt.title('Silhouette Score as a Function of Number of Clusters')
plt.xlabel('Number of K-means Clusters')
plt.ylabel('Silhouette Score')
plt.show()

"""The plot above shows that at 30 clusters we cease to gain meaningful improvement in silhouette score by modeling additional clusters. This is an important visual as it suggests that we might best break our customer set into (at most) 30 distinct groups from which to evaluate and target according to their number, habits, needs, etc. to better serve them and ultimately increase sales. We reach the highest Silhouette score at 50 clusters, but the difference in score by adding 20 more clusters is negligible and these additional 20 clusters might not tell us anything about our customer base.

**Heirachrical Clustering**
"""

plt.figure(figsize=(27,10))
plt.subplot(1,3,1)
plt.title("Dendogram with linkage method: complete")
dendrogram(linkage(X_std, method='complete'))

plt.subplot(1,3,2)
plt.title("Dendogram with linkage method: average")
dendrogram(linkage(X_std, method='average'))

plt.subplot(1,3,3)
plt.title("Dendogram with linkage method: ward")
dendrogram(linkage(X_std, method='ward'))

plt.show()

# Defining the agglomerative clustering with n_clusters=30 from the K-means Silhouette score results 
agg_cluster_complete = AgglomerativeClustering(linkage='complete', affinity='cosine', n_clusters=30)
agg_cluster_average = AgglomerativeClustering(linkage='average', affinity='cosine', n_clusters=30)
agg_cluster_ward = AgglomerativeClustering(linkage='ward', affinity='euclidean', n_clusters=30)

# Fit models with the three linkage methods
clusters_complete = agg_cluster_complete.fit_predict(X_std)
clusters_average = agg_cluster_average.fit_predict(X_std)
clusters_ward = agg_cluster_ward.fit_predict(X_std)

print(f"Silhouette score of linkage method(complete): {metrics.silhouette_score(X_std, clusters_complete, metric='euclidean')}")
print(f"Silhouette score of linkage method(average): {metrics.silhouette_score(X_std, clusters_average, metric='euclidean')}")
print(f"Silhouette score of linkage method(ward): {metrics.silhouette_score(X_std, clusters_ward, metric='euclidean')}")

"""Based on the Silhouette scores from the K-means clustering algorithm indicating 30 clusters is optimal, we ran the AgglomerativeClustering algorithm at 30 clusters for the three linkage methods as an initial trial and found it to be the optimal parameter in terms of scoring. The best Silhouette score of the three linkage methods comes from the Ward method. The Ward method dendogram depicts a relatively uniform cluster size distribution when compared to the other two linkage methods. This is a common result for the Ward method.

**DBSCAN Clustering**

Iterating on eps:
"""

epsilon = [value for value in np.arange(0.50, 2.25, 0.25)]

for value in epsilon:
  
  dbscan_cluster = DBSCAN(eps=value, min_samples=2, metric='euclidean')

  clusters = dbscan_cluster.fit_predict(X_std)
  print(f"The silhouette score of the DBSCAN solution(eps={value}): {metrics.silhouette_score(X_std, clusters, metric='euclidean')}")

"""Iterating on min_samples:"""

samples = [value for value in range(1, 11)]

for value in samples:
  dbscan_cluster = DBSCAN(eps=1, min_samples=value, metric='euclidean')

  clusters = dbscan_cluster.fit_predict(X_std)
  print(f"The silhouette score of the DBSCAN solution(min_samples={value}): {metrics.silhouette_score(X_std, clusters, metric='euclidean')}")

"""The above loops iterate across ranges of eps and min_samples to find optimal values. The highest silhouette score obtained is with eps=1 and min_samples=2. We will plot the 2-D visual of this parameter combination below. The DBSCAN algorithm, while providing the highest Silhoutee score, is not ideal for our goal with this project as it divides the datapoints in to too many groups. For our business purpose of wanting to identify unique customer groups and what attributes they posses, subdividing our 2000 customers into 80+ groups would leave us unfocused in our ability to better target their needs as that is too granular to be effective."""

# Defining the agglomerative clustering
dbscan_cluster = DBSCAN(eps=1, min_samples=2, metric='euclidean')

# Fit model
clusters = dbscan_cluster.fit_predict(X_std)

# Visualize
pca = PCA(n_components=2).fit_transform(X_std)

plt.figure(figsize=(10,5))
colours = ["r","b","g"]
for i in range(pca.shape[0]):
    plt.text(pca[i, 0], pca[i, 1], str(clusters[i]),
             fontdict={'weight': 'bold', 'size': 40}
        )

plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.show()

print("The silhouette score of the DBSCAN solution: {}"
      .format(metrics.silhouette_score(X_std, clusters, metric='euclidean')))

"""The 2-D representation of the clusters suggests 81 unique clusters from our 2,000 observation dataset. This seems pretty granular when you consider that it is a customer dataset with 7 features. Each cluster would be made up of an average of 25 customers. 

According to the Silhouette score, this DBSCAN model is the best one yet as it outperforms the best K-means and Heirarchical models. That being said, we know that Silhouette score as our only metric can mislead us, and reconciling 80 distinct clusters in a customer set of 2,000 would be impractical to implement in a business sense so for this reason we will focus on other algorithms for best practice modeling.

**Gaussian Mixture Models Clustering**
"""

# Defining the agglomerative clustering
gmm_cluster = GaussianMixture(n_components=40, random_state=12)

# Fit model
clusters = gmm_cluster.fit_predict(X_std)

print("The silhouette score of the GMM solution: {}"
      .format(metrics.silhouette_score(X_std, clusters, metric='euclidean')))

# Defining the agglomerative clustering

types = ['full', 'tied', 'diag', 'spherical']

for type in types:

  gmm_cluster = GaussianMixture(n_components=40, covariance_type=type, random_state=12)
  
  # Fit model
  clusters = gmm_cluster.fit_predict(X_std)

  print("The silhouette score of the GMM solution of type {}: {}"
      .format(type, metrics.silhouette_score(X_std, clusters, metric='euclidean')))

pca = PCA(n_components=2).fit_transform(X_std)

plt.figure(figsize=(10,5))
colours = 'rbg'
for i in range(pca.shape[0]):
    plt.text(pca[i, 0], pca[i, 1], str(clusters[i]),
             fontdict={'weight': 'bold', 'size': 50}
        )

plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.show()

"""The geometry independence that GMM models serves it well in that it handles non-linearity well. The GMM algorithm with 40 components scores similarly to the other algorithms in the 0.42 range for Silhouette score. Visualizing the clusters above, GMM does a pretty effective job at clustering with tis dataset as the discreet numbers can be seen tightly grouped together in large part. The clusters tend to bunch and overlap - particularly in the middle of the plot which leaves something to be desired.

# Selecting the Best Algorithm and Understanding our Customer Groups

**K-means Clusters Breakdown**
"""

preds = KMeans(n_clusters=30, random_state=12).fit_predict(X_std)

print(preds)

customers_dropID = customers.drop('ID', axis=1)
customers_dropID['Kmeans_clusters'] = preds
customers_dropID.sort_values('income', ascending=False)
customers_dropID.groupby('Kmeans_clusters').describe()

"""**Visualizing Average Feature Values by Cluster**

"""

features= ['sex',	'marital_status',	'age',	'education',	'income',	'occupation',	'city_size']
clusters = customers_dropID.groupby('Kmeans_clusters')
plt.figure(figsize=(27,15))

for i, feature in enumerate(features):
  fig = plt.figure()
  ax = fig.add_axes([0,0,1,1])
  ax.bar(customers_dropID.Kmeans_clusters,customers_dropID[feature])
  plt.title(f"Mean {feature} per Cluster Group")
  plt.xlabel('Cluster #')
  plt.ylabel(f"Mean {feature}")

plt.show()

"""The above bar graphs summarize the mean feature value of each individual K-means cluster allowing the analyst to build profiles on the clusters on how best to make business changes according to customer tendencies of the various groups.

# Summary

In this work we evaluated a customer dataset without ground truth by employing a comprehensive set of clustering algorithms and dimensionality reduction techniques that allowed us to converge on a seemingly reasonable breakdown of its constituents into 'like' groups. Among the 2000 observations, or customers, we achieve the best statistical scores when they are subdivided into 30 to 40 subgroups. On average, each subgroup would have 50-65 customers in it from which to tailor marketing strategy, physical accomodations and other considerations. Gathering further data on this customer set like shopping and spending history could further improve the effort to develop solutions for improving customer service, inventory, marketing and ultimately sales. The 30 K-means clusters model was chosen for its performance and practicality in understanding the customer base to better target groups and their potential purchasing preferences and how best to market both prodcuts and select appropriate marketing mediums.
"""
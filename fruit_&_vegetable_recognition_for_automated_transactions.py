# -*- coding: utf-8 -*-
"""Fruit & Vegetable Recognition For Automated Transactions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/178DuHgx8MKVLl50kGqnjqj6Q11G1fWF5

# Objective

The objective of this project is to build and train a CNN to be able to identify various fruit and vegetable images and classify them into their respective categories 
with a high degree of accuracy. The value in this project could be realized through implementation at a grocery store checkout counter, both at a self-checkouts and 
traditional ones manned by a cashier. When fully trained with a large set of high-quality images from consistent camera qualities, angles and distances, this CNN model
(which was trained without such convenient consistencies) is evidence that it can deliver a solution that reliably automates the process of identifying the type of 
produce that is being purchased so as to eliminate human errors while reducing the transaction time. The idea is that the produce would be placed on the scale with a 
camera mounted above or to the side next to the barcode scanner and an image would be captured and the produce would be classified and weighed simultaneously such that
a price could be logged on the transaction sheet.

If the produce is not sold by the pound, but by the count, an additional network would need to be built that recognizes the items themselves and also the count. 
Further issues that would need to be addressed before rollout would be handling the bags that contain the produce items and their impact on the image recognition. 
The opaque polyethylene bags that most commonly carry produce would likely need to be changed to a mesh bag with large enough hole sizes to reveal the prominent item 
features. Assuming this could be done cost-effectively, the CNN would be able to ignore the mesh pattern features with enough training where all the items would have 
this same pattern.

  
The following is a list of the 10 fruit and vegetable items:


*   Apple
*   Banana
*   Carrot
*   Grapes
*   Onion
*   Orange
*   Pineapple
*   Potato
*   Tomato
*   Watermelon

**Mounting Drive to access images dataset**
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=False)

"""**Importing libraries**"""

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K
from keras.optimizers import SGD
from keras import applications
from tensorflow.keras.applications.vgg16 import VGG16
import keras.callbacks
from keras.callbacks import ModelCheckpoint
import keras
from keras.layers import InputLayer
from keras.callbacks import ModelCheckpoint

import tensorflow as tf

import numpy as np
import matplotlib as plt
from sklearn.metrics import classification_report as cr
from sklearn.metrics import confusion_matrix as cm

!pip install pillow
import sys
from PIL import Image

sys.modules['Image'] = Image

from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

"""**Setting image size, providing path to images directory and defining image variables**"""

# dimensions of the images.
img_width = 150
img_height = 150

top_model_weights_path = 'drive/MyDrive/Thinkful/bottleneck_fc_model_ljr.h5'
train_data_dir = 'drive/MyDrive/Thinkful/fruit_vegetable_image_recognition_latest/train_a'
validation_data_dir = 'drive/MyDrive/Thinkful/fruit_vegetable_image_recognition_latest/validation_a'
nb_train_samples = 9644
nb_validation_samples = 1802
epochs = 30
batch_size = 10

"""In my final dataset there are 9644 images in the training set and 1802 images in the validation set balanced across 10 unique classes of fruits and vegetables. 
These images were downloaded from various websites."""

if K.image_data_format() == 'channels_last':
    input_shape = (img_width, img_height, 4)
else:
    input_shape = (4, img_width, img_height)

"""**Incorporating the VGG16 network for transfer learning**"""

def save_bottleneck_features():
    datagen = ImageDataGenerator(rescale=1. / 255)

    # build the VGG16 network
    model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', classes=10)

    train_generator = datagen.flow_from_directory(
        train_data_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode=None,
        shuffle=False)
    bottleneck_features_train = model.predict(
        train_generator, nb_train_samples // batch_size)
    np.save(open('bottleneck_features_train.npy', 'wb'),
            bottleneck_features_train)

    validation_generator = datagen.flow_from_directory(
        validation_data_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode=None,
        shuffle=False)
    bottleneck_features_validation = model.predict(
        validation_generator, nb_validation_samples // batch_size)
    np.save(open('bottleneck_features_validation.npy', 'wb'),
            bottleneck_features_validation)

save_bottleneck_features()

"""Implementing transfer learning from the VGG16 network improved accuracies more than 70% over the base CNN network accuracies! By leveraging the VGG16 network my CNN
model was given a head start on recognizing image features. The following text describes VGG16: 

"VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks 
for Large-Scale Image Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes.

The input to cov1 layer is of fixed size 224 x 224 RGB image. The image is passed through a stack of convolutional (conv.) layers, where the filters were used with a 
very small receptive field: 3×3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations, it also utilizes 1×1 
convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the 
spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1-pixel for 3×3 conv. layers. Spatial 
pooling is carried out by five max-pooling layers, which follow some of the conv.  layers (not all the conv. layers are followed by max-pooling). Max-pooling is 
performed over a 2×2 pixel window, with stride 2.

Three Fully-Connected (FC) layers follow a stack of convolutional layers (which has a different depth in different architectures): the first two have 4096 channels 
each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration 
of the fully connected layers is the same in all networks.

All hidden layers are equipped with the rectification (ReLU) non-linearity. It is also noted that none of the networks (except for one) contain Local Response 
Normalisation (LRN), such normalization does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time."

**VGG16 Architecture**


Image source: https://neurohive.io/en/popular-networks/vgg16/

**Training the model from the VGG16 weights**
"""

def plot_confusion_matrix(cm, class_names):
    """
    Returns a matplotlib figure containing the plotted confusion matrix.
    
    Args:
       cm (array, shape = [n, n]): a confusion matrix of integer classes
       class_names (array, shape = [n]): String names of the integer classes
    """
    
    figure = plt.figure(figsize=(8, 8))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title("Confusion matrix")
    plt.colorbar()
    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names, rotation=45)
    plt.yticks(tick_marks, class_names)
    
    # Normalize the confusion matrix.
    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)
    
    # Use white text if squares are dark; otherwise black.
    threshold = cm.max() / 2.
    
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        color = "white" if cm[i, j] > threshold else "black"
        plt.text(j, i, cm[i, j], horizontalalignment="center", color=color)
        
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    return figure

import datetime
datetime.datetime.now()

logdir = "logs/image/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = keras.callbacks.TensorBoard(log_dir = logdir, histogram_freq = 1)
file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')

def plot_to_image(figure):
    """
    Converts the matplotlib plot specified by 'figure' to a PNG image and
    returns it. The supplied figure is closed and inaccessible after this call.
    """
    
    buf = io.BytesIO()
    
    # Use plt.savefig to save the plot to a PNG in memory.
    plt.savefig(buf, format='png')
    
    # Closing the figure prevents it from being displayed directly inside
    # the notebook.
    plt.close(figure)
    buf.seek(0)
    
    # Use tf.image.decode_png to convert the PNG buffer
    # to a TF image. Make sure you use 4 channels.
    image = tf.image.decode_png(buf.getvalue(), channels=4)
    
    # Use tf.expand_dims to add the batch dimension
    image = tf.expand_dims(image, 0)
    
    return image

def log_confusion_matrix(epoch, logs):
    
    # Use the model to predict the values from the test_images.
    test_pred_raw = model.predict(test_images)
    
    test_pred = np.argmax(test_pred_raw, axis=1)
    
    # Calculate the confusion matrix using sklearn.metrics
    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)
    
    figure = plot_confusion_matrix(cm, class_names=class_names)
    cm_image = plot_to_image(figure)
    
    # Log the confusion matrix as an image summary.
    with file_writer_cm.as_default():
        tf.summary.image("Confusion Matrix", cm_image, step=epoch)

cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)

callbacks = [tensorboard_callback, cm_callback]

def train_top_model():
    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))
    train_labels = np.array(
        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))

    validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))
    validation_labels = np.array(
        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))
    

    model = Sequential()
    model.add(Flatten(input_shape=train_data.shape[1:]))
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(10, activation='softmax'))

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    model.fit(train_data, train_labels,
              epochs=epochs,
              batch_size=batch_size,
              callbacks=tensorboard_callback,
              validation_data=(validation_data, validation_labels))
    model.save_weights('drive/MyDrive/Thinkful/bottleneck_fc_model_ljr.h5')


train_top_model()

"""The final CNN model stacked on top of the VGG network learning yields a fairly strong accuracy result considering the relatively dataset that was used. Roughly 
1000 images per class on a 10-class dataset made up the training set. Roughly 180 images per class made up the validation set. All of the network parameters were 
tweaked by hand to obtain the optimal result and the final model reflects the best result achievable with the current dataset - 84% test/validation accuracy. The 
InceptionV3 network was also used on this dataset and yielded almost identical accuracy values as the VGG16 network."""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# Start TensorBoard.
# %tensorboard --logdir logs/image
# Train the classifier.

"""We can see from the above loss chart that our validation set loss is increasing with each epoch but the accuray stays relatively flat after 10 epochs. This is 
likely the result of overfitting the model to the training set. With an increase in training image data and/or a more focused set of images with consistent backgrounds, 
camera angle and focus, the validation set would likely score much higher accuracies - even approaching 100%. If the image dataset is derived from the actual cameras 
that will be capturing the images in real time at the checkout then the dataset will be strongly tailored to the purpose and can be expected to be a very effective 
automated tool."""
